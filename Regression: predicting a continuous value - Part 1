(Source: https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab)

Regression: predicting a continuous value
Regression predicts a continuous target variable Y. 
It allows you to estimate a value, such as housing prices or human lifespan, based on input data X

Here, target variable means the unknown variable we care about predicting, 
and continuous means there aren’t gaps (discontinuities) in the value that Y can take on. 
A person’s weight and height are continuous values. 
Discrete variables, on the other hand, can only take on a finite number of values — 
for example, the number of kids somebody has is a discrete variable

Predicting income is a classic regression problem. 
Your input data X includes all relevant information about individuals in the data set that can be used to predict income, 
such as years of education, years of work experience, job title, or zip code. 

These attributes are called features, which can be numerical (e.g. years of work experience) or 
categorical (e.g. job title or field of study)

The data is split into a training data set and a test data set. 
The training set has labels, so your model can learn from these labeled examples. 
The test set does not have labels, i.e. you don’t yet know the value you’re trying to predict

Regression
Y = f(X) + ϵ, where X = (x1, x2…xn)

Training: machine learns f from labeled training data
Test: machine predicts Y from unlabeled testing data

In our trivially simple 2D example, this could take the form of a .csv file 
where each row contains a person’s education level and income. 
Add more columns with more features and you’ll have a more complex, but possibly more accurate, model

We have our data set X, and corresponding target values Y. 
The goal of ordinary least squares (OLS) regression is to learn a linear model that we can use to predict 
a new y given a previously unseen x with as little error as possible

X_train = [4, 5, 0, 2, …, 6] # years of post-secondary education
Y_train = [80, 91.5, 42, 55, …, 100] # corresponding annual incomes, in thousands of dollars

Linear regression is a parametric method, which means it makes an assumption about the form of the function relating X and Y

            y = β0 + β1*x + E (Error term)
            
we make the explicit assumption that there is a linear relationship between X and Y 

β0 is the y-intercept and β1 is the slope of our line

Our goal is to learn the model parameters (in this case, β0 and β1) that minimize error in the model’s predictions
