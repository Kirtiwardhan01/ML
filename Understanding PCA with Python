(Source: https://towardsdatascience.com/dive-into-pca-principal-component-analysis-with-python-43ded13ead21)
 
 Understanding PCA with Python
 Feeling lost in deciding which features to choose so that your model is safe from overfitting?
 Well, PCA can surely in that case
 
 In this meditation we will go through a simple explanation of principal component analysis on cancer data-set and 
 see examples of feature space dimension reduction to data visualization
 
import pandas as pd
import numpy as np
import matplotlib as mpl
import seaborn as sns

from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

Sometimes it’s better to know about the data that you’re using and 
we can use DESCR to know the basic description of the data-set

print(cancer.DESCR)

From this you now know that this data-set has 30 features like smoothness, radius etc. The number of instances are 569 and 
out of them 212 are malignant and 357 are benign. The target variables are listed as 0 and 1

To know more about how the features affect the target, we can plot histograms of malignant and benign classes. 
If the two histograms are separated based on the feature, then we can say that the feature is important to discern the instances

import numpy as np
import matplotlib.pyplot as plt 
# from matplotlib.pyplot import matplotlib

fig,axes =plt.subplots(10,3, figsize=(12, 9)) # 3 columns each containing 10 figures, total 30 features
malignant=cancer.data[cancer.target==0] # define malignant
benign=cancer.data[cancer.target==1] # define benign
ax=axes.ravel()# flat axes with numpy ravel

for i in range(30):
  _,bins=np.histogram(cancer.data[:,i],bins=40)
  ax[i].hist(malignant[:,i],bins=bins,color='r',alpha=.5)# red color for malignant class
  ax[i].hist(benign[:,i],bins=bins,color='g',alpha=0.3)# alpha is for transparency in the overlapped region 

  ax[i].set_title(cancer.feature_names[i],fontsize=9)
  ax[i].axes.get_xaxis().set_visible(False) # the x-axis co-ordinates are not so useful, 
#as we just want to look how well separated the histograms are
 
ax[i].set_yticks(())
ax[0].legend(['malignant','benign'],loc='best',fontsize=8)
plt.tight_layout()# let's make good plots
plt.show()

#Before using PCA to these cancer data-set, let’s understand very simply what PCA actually does
#We know that in a data-set there are high possibilities for some features to be correlated
#Let’s see some example plots from cancer data set 

cancer_df=pd.DataFrame(cancer.data,columns=cancer.feature_names)# just convert the scikit learn data-set to pandas data-frame.
plt.subplot(1,2,1)#first plot

plt.scatter(cancer_df['worst symmetry'], cancer_df['worst texture'], s=cancer_df['worst area']*0.05, 
            color='magenta', label='check', alpha=0.3)
plt.xlabel('Worst Symmetry',fontsize=12)
plt.ylabel('Worst Texture',fontsize=12)

plt.subplot(1,2,2)# 2nd plot
plt.scatter(cancer_df['mean radius'], cancer_df['mean concave points'], s=cancer_df['mean area']*0.05, 
            color='purple', label='check', alpha=0.3)
plt.xlabel('Mean Radius',fontsize=12)
plt.ylabel('Mean Concave Points',fontsize=12)
plt.tight_layout()
plt.show()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

## compute the mean and standard which will be used in the next command
X_scaled = scaler.fit_transform(cancer.data)
X_scaled

## we can check the minimum and maximum of the scaled features which we expect to be 0 and 1
print("after scaling minimum", X_scaled.min(axis=0))
print("after scaling maximum", X_scaled.max(axis=0))

