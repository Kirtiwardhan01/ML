PCA is essentially a method that reduces the dimension of the feature space in such a way that new variables are orthogonal to each other 
(i.e. they are independent or not correlated)

Before applying PCA, we scale our data such that each feature has unit variance. 
This is necessary because fitting algorithms highly depends on the scaling of the features. 

Here we use the StandardScalermodule 
for scaling the features individually
StandardScalersubtracts the mean from each features and then scale to unit variance

Now we’re ready to apply PCA on this scaled data-set. 
We start as before with StandardScaler, where we instantiate, then fit and finally transform the scaled data. 
While applying PCA you can mention how many principal components you want to keep

Why PCA rather than just feature analysis ? (Answer hints: large data-set, many features, let’s reduce dimension of feature space)
We started our example with cancer data-set and found 30 features with 2 classes.
To apply PCA on this data-set, first we scale all the features and then apply fit_transformmethod of PCA (with 3 principal components) 
on the scaled features.
We show that out of those 3 principal components, 2 contribute to 87% of the total variance.
Based on these 2 principal components we visualize the data and see very clear separation between ‘Malignant’ and ‘Benign’ classes

One final word of caution
When performing PCA, it is typically a good idea to normalize the data first. 
Because PCA seeks to identify the principal components with the highest variance,
if the data is not properly normalized, attributes with large values and large variances (in absolute terms) 
will end up dominating the first principal component when they should not. 
Normalizing the data gets each attribute onto more or less the same scale, 
so that each attribute has an opportunity to contribute to the principal component analysis.

